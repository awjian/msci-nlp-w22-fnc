{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc749cdf-c5de-4090-ba05-ae60c1e33ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation, Flatten, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5772b3d4-77d7-4272-8c62-dba887060227",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a741ec7-5b28-47e8-b88f-fe2a72369e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels = to_categorical(np.array(train['Stance']))\n",
    "testing_labels = to_categorical(np.array(test['Stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b972f259-a0c0-4b69-bf28-5acdb3e28d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['combinedText'])\n",
    "train_sequence = tokenizer.texts_to_sequences(train['combinedText'])\n",
    "test_sequence = tokenizer.texts_to_sequences(test['combinedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6fb5002-8d7d-4b84-9ee0-8eef8115dbe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cba04470-d93b-4e2b-87d7-b099b8ec41e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4900"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longest_sequences = [len(x) for x in (train_sequence)]\n",
    "# longest_sequence = max(longest_sequences)\n",
    "# longest_sequence = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5958c5dc-b621-4afa-a3f2-060c654507ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pad = pad_sequences(train_sequence, maxlen=longest_sequence, padding='post', truncating='post')\n",
    "# test_pad = pad_sequences(test_sequence, maxlen=longest_sequence, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a1ad8e-549e-4369-8ad7-d2bbed113a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "in_file = '../data/glove/glove.6B.200d.txt'\n",
    "out_file = '../data/glove.200d.word2vec.txt'\n",
    "\n",
    "glove2word2vec(in_file, out_file)\n",
    "w2v = KeyedVectors.load_word2vec_format(out_file, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feacb695-e41b-4690-8026-5cc7f2aaf9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index.keys()\n",
    "# Add one because index 0 is reserved and isn't assigned to any word\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "embedding_matrix = np.zeros((len(vocab)+1, embedding_dim))\n",
    "\n",
    "embedding_matrix[0] = np.random.random((1, embedding_dim))\n",
    "for i, word in enumerate(vocab, 1):\n",
    "    try:\n",
    "        embedding_matrix[i] = w2v[word]\n",
    "    except KeyError as e:\n",
    "        embedding_matrix[i] = np.random.random((1, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0f044dd-7733-4145-8b6e-ae49091f0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pad, val_pad, train_labels, val_labels = train_test_split(train_pad, train_labels, random_state = 42, test_size = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f26de-d670-45d0-a6de-56bdaa21dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune: \n",
    "# height/width: 3x3, 5x5, 7x7\n",
    "# MaxPooling vs Max-Over-Time Pooling\n",
    "# Different drop-out rates\n",
    "# Seqeuence length\n",
    "kernel_sizes = [3, 5, 7]\n",
    "# pooling_method = [MaxPooling1D(pool_size=2)]\n",
    "dropouts = [0.2, 0.3, 0.4]\n",
    "sequences = [100, 200, 300, 500] \n",
    "\n",
    "best_model = Sequential()\n",
    "best_accuracy = 0\n",
    "for sequence_length in sequences:\n",
    "    train_pad = pad_sequences(train_sequence, maxlen=sequence_length, padding='post', truncating='post')\n",
    "    test_pad = pad_sequences(test_sequence, maxlen=sequence_length, padding='post', truncating='post')\n",
    "    train_pad, val_pad, train_labels, val_labels = train_test_split(train_pad, training_labels, random_state = 42, test_size = 0.15)\n",
    "    for kernel in kernel_sizes:\n",
    "        for dropout in dropouts:\n",
    "            keras.backend.clear_session()\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                                output_dim=embedding_dim,\n",
    "                                embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                                input_length=sequence_length,\n",
    "                                trainable=True,\n",
    "                                name='embedding_layer',\n",
    "                                ))\n",
    "            model.add(Conv1D(filters=256, kernel_size=kernel, activation='relu'))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(Conv1D(filters=128, kernel_size=kernel, activation='relu'))\n",
    "            model.add(MaxPooling1D(pool_size=2))\n",
    "            model.add(Conv1D(filters=64, kernel_size=kernel, activation='relu'))\n",
    "            model.add(GlobalMaxPooling1D())\n",
    "#             model.add(Flatten())\n",
    "            model.add(Dense(embedding_dim, activation='relu'))\n",
    "            model.add(Dropout(dropout))\n",
    "            model.add(Dense(4, activation='softmax'))\n",
    "#             print(model.summary())\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            model.fit(train_pad, train_labels, epochs=10, batch_size=200, validation_data=(val_pad, val_labels))\n",
    "\n",
    "            _, accuracy = model.evaluate(test_pad, testing_labels, batch_size=200)\n",
    "            print(\"Test Set Accuracy = {:.4f}\".format(accuracy))\n",
    "\n",
    "            if (accuracy > best_accuracy):\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                best_model_string = f\"Best Model has seq_length={sequence_length}, kernel={kernel} and dropout={dropout}\"\n",
    "                print(best_model_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6550c645-cce3-4456-affb-0cf8ccadbb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summary :75.0 % of the summaries have a length less than or equal to 478.0\n"
     ]
    }
   ],
   "source": [
    "word_count = lambda x:len(x.split())\n",
    "train['text_wc'] = train['combinedText'].apply(word_count)\n",
    "\n",
    "p = 75.0\n",
    "\n",
    "print(' Summary :{} % of the summaries have a length less than or equal to {}'.format(p, np.percentile(train['text_wc'], p)))\n",
    "MAX_LEN = int(np.percentile(train['text_wc'], p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f3c61c-74e9-4030-9f80-d2af252f95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 36545, 1: 18272, 2: 18272, 0: 9136})\n",
      "Combined:Counter({1: 18272, 2: 18272, 3: 18272, 0: 9136})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy={0:round(36545*0.25), 1: round(36545*0.5), 2: round(36545*0.5)})\n",
    "undersample = RandomUnderSampler(sampling_strategy={3: round(36545*0.5)})\n",
    "\n",
    "train_over, training_over_labels = oversample.fit_resample(train, train['Stance'])\n",
    "\n",
    "print(Counter(training_over_labels))\n",
    "\n",
    "train_combined_sampling, training_combined_sampling_labels = undersample.fit_resample(train_over, training_over_labels)\n",
    "\n",
    "print(f\"Combined:{Counter(training_combined_sampling_labels)}\")\n",
    "\n",
    "training_combined_labels = to_categorical(np.array(training_combined_sampling_labels))\n",
    "testing_labels = to_categorical(np.array(test['Stance']))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['combinedText'])\n",
    "test_sequence = tokenizer.texts_to_sequences(test['combinedText'])\n",
    "train_combined_sequence = tokenizer.texts_to_sequences(train_combined_sampling['combinedText'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1f4c9-cb32-4fbd-89bb-2926b8436e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_pad = pad_sequences(train_combined_sequence, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(test_sequence, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "train_combined_pad, val_combined_pad, train_combined_labels, val_combined_labels = train_test_split(train_combined_pad, training_combined_labels, random_state = 42, test_size = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad44bed2-ae42-4090-8a82-aa55dc2bfa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model is: kernel=3, dropout=0.2\n",
      "WARNING:tensorflow:From /srv/jupyter_python3-extras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /srv/jupyter_python3-extras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 54359 samples, validate on 9593 samples\n",
      "Epoch 1/10\n",
      "54359/54359 [==============================] - 77s 1ms/step - loss: 0.8742 - accuracy: 0.6226 - val_loss: 0.4797 - val_accuracy: 0.8248\n",
      "Epoch 2/10\n",
      "54359/54359 [==============================] - 77s 1ms/step - loss: 0.3464 - accuracy: 0.8742 - val_loss: 0.2611 - val_accuracy: 0.9061\n",
      "Epoch 3/10\n",
      "54359/54359 [==============================] - 77s 1ms/step - loss: 0.2038 - accuracy: 0.9280 - val_loss: 0.1840 - val_accuracy: 0.9351\n",
      "Epoch 4/10\n",
      "54359/54359 [==============================] - 76s 1ms/step - loss: 0.1281 - accuracy: 0.9557 - val_loss: 0.1655 - val_accuracy: 0.9414\n",
      "Epoch 5/10\n",
      "54359/54359 [==============================] - 86s 2ms/step - loss: 0.0921 - accuracy: 0.9684 - val_loss: 0.1382 - val_accuracy: 0.9567\n",
      "Epoch 6/10\n",
      " 4800/54359 [=>............................] - ETA: 1:51 - loss: 0.0652 - accuracy: 0.9783"
     ]
    }
   ],
   "source": [
    "kernel_sizes = [3, 5, 7]\n",
    "dropouts = [0.2, 0.3, 0.4]\n",
    "\n",
    "best_model = Sequential()\n",
    "best_accuracy = 0\n",
    "for kernel in kernel_sizes:\n",
    "    for dropout in dropouts:\n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        print(f\"Current Model is: kernel={kernel}, dropout={dropout}\")\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=MAX_LEN,\n",
    "                            trainable=True,\n",
    "                            name='embedding_layer',\n",
    "                            ))\n",
    "        model.add(Conv1D(filters=256, kernel_size=kernel, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=128, kernel_size=kernel, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=64, kernel_size=kernel, activation='relu'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        #             model.add(Flatten())\n",
    "        model.add(Dense(embedding_dim, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        #             print(model.summary())\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(train_combined_pad, train_combined_labels, epochs=10, batch_size=200, validation_data=(val_combined_pad, val_combined_labels))\n",
    "\n",
    "        _, accuracy = model.evaluate(test_pad, testing_labels, batch_size=200)\n",
    "        print(\"Test Set Accuracy = {:.4f}\".format(accuracy))\n",
    "\n",
    "        if (accuracy > best_accuracy):\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            best_model_string = f\"Best Model has kernel={kernel} and dropout={dropout}\"\n",
    "            print(best_model_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99282da-2023-4f14-99b8-a4c33c974acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
