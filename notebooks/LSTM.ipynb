{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc749cdf-c5de-4090-ba05-ae60c1e33ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation, Flatten, LSTM, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea6e6a4-9634-4f13-b0f2-865b0e665fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some hyperparameters that can be tuned\n",
    "MAX_SENT_LEN = 478 # Determined in CNN file\n",
    "MAX_VOCAB_SIZE = 100000\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 1000\n",
    "N_EPOCHS = 20\n",
    "DROPOUT = 0.0001\n",
    "L2 = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5772b3d4-77d7-4272-8c62-dba887060227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../fnc-1-baseline-master/data/train.csv')\n",
    "# test = pd.read_csv('../fnc-1-baseline-master/data/test.csv')\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a741ec7-5b28-47e8-b88f-fe2a72369e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(np.array(train['Stance']))\n",
    "test_labels = to_categorical(np.array(test['Stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b972f259-a0c0-4b69-bf28-5acdb3e28d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train['combinedText'])\n",
    "train_sequence = tokenizer.texts_to_sequences(train['combinedText'])\n",
    "test_sequence = tokenizer.texts_to_sequences(test['combinedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba04470-d93b-4e2b-87d7-b099b8ec41e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5958c5dc-b621-4afa-a3f2-060c654507ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(test_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "\n",
    "# pad sequences manually\n",
    "# def preprocessing(headlines, bodies, stances, tokenizer):\n",
    "#     # Convert the sequence of words to sequnce of indices\n",
    "#     X = tokenizer.texts_to_sequences([' '.join((headline + \"<>\" + body)[:MAX_SENT_LEN]) for headline in headlines for body in bodies])\n",
    "#     X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "#     return X, y\n",
    "\n",
    "# X_train, y_train = preprocessing(headlines, bodies, stance, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6a1ad8e-549e-4369-8ad7-d2bbed113a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "in_file = '../fnc-1-baseline-master/data/glove_wiki/glove.6B.100d.txt'\n",
    "out_file = '../fnc-1-baseline-master/data/glove_wiki/glove.6B.100d.word2vec.txt'\n",
    "\n",
    "glove2word2vec(in_file, out_file)\n",
    "w2v = KeyedVectors.load_word2vec_format(out_file, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8b3c5fa-cedf-4895-ad75-bcaef4d0ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = '../fnc-1-baseline-master/data/glove_twitter/glove.27B.200d.txt'\n",
    "# outp = '../fnc-1-baseline-master/data/glove_twitter/glove.27B.200d.word2vec.txt'\n",
    "\n",
    "# glove2word2vec(inp, outp)\n",
    "# w2v_twitter = KeyedVectors.load_word2vec_format(outp, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "feacb695-e41b-4690-8026-5cc7f2aaf9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index.keys()\n",
    "# Add one because index 0 is reserved and isn't assigned to any word\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "embedding_matrix = np.zeros((len(vocab)+1, EMBEDDING_DIM))\n",
    "\n",
    "embedding_matrix[0] = np.random.random((1, EMBEDDING_DIM))\n",
    "for i, word in enumerate(vocab, 1):\n",
    "    try:\n",
    "        embedding_matrix[i] = w2v[word]\n",
    "    except KeyError as e:\n",
    "        embedding_matrix[i] = np.random.random((1, EMBEDDING_DIM))\n",
    "        \n",
    "# from a3\n",
    "# embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM))   \n",
    "# for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
    "#     try:\n",
    "#         embeddings_vector = word_embeddings[word]\n",
    "#     except KeyError:\n",
    "#         embeddings_vector = None\n",
    "#     if embeddings_vector is not None:\n",
    "#         embeddings_matrix[i] = embeddings_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b0f044dd-7733-4145-8b6e-ae49091f0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad, val_pad, train_labels, val_labels = train_test_split(train_pad, train_labels, random_state = 42, test_size = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fa6a8cf-aa4f-478f-8446-863ca753baa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6372"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa6f26de-d670-45d0-a6de-56bdaa21dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " word_embedding_layer (Embed  (None, 30, 100)          2787600   \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 240)              212160    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 240)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 240)               0         \n",
      "                                                                 \n",
      " softmax_output_layer (Dense  (None, 4)                964       \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,724\n",
      "Trainable params: 213,124\n",
      "Non-trainable params: 2,787,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length = MAX_SENT_LEN,\n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable=False, \n",
    "                    name='word_embedding_layer',\n",
    "                    mask_zero=True))\n",
    "\n",
    "model.add(Bidirectional(LSTM(120, return_sequences = False)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Dense(4, activation = 'softmax', name='softmax_output_layer'))\n",
    "\n",
    "# model.add(Dense(2, activation = 'softmax', name='softmax_output_layer', activity_regularizer=l2(L2)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bec353d6-f337-45d7-aa03-2d7bb6bae268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "43/43 [==============================] - 16s 258ms/step - loss: 0.8021 - accuracy: 0.7302 - val_loss: 0.7522 - val_accuracy: 0.7393\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 9s 216ms/step - loss: 0.7433 - accuracy: 0.7366 - val_loss: 0.7095 - val_accuracy: 0.7488\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.6884 - accuracy: 0.7495 - val_loss: 0.6560 - val_accuracy: 0.7680\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 9s 212ms/step - loss: 0.6145 - accuracy: 0.7700 - val_loss: 0.5753 - val_accuracy: 0.7876\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 10s 221ms/step - loss: 0.5268 - accuracy: 0.8025 - val_loss: 0.5002 - val_accuracy: 0.8100\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 9s 218ms/step - loss: 0.4587 - accuracy: 0.8273 - val_loss: 0.4554 - val_accuracy: 0.8306\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 9s 220ms/step - loss: 0.3887 - accuracy: 0.8559 - val_loss: 0.3977 - val_accuracy: 0.8514\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 10s 223ms/step - loss: 0.3334 - accuracy: 0.8771 - val_loss: 0.3563 - val_accuracy: 0.8665\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 10s 227ms/step - loss: 0.2953 - accuracy: 0.8905 - val_loss: 0.3360 - val_accuracy: 0.8766\n",
      "Epoch 10/20\n",
      "43/43 [==============================] - 10s 224ms/step - loss: 0.2585 - accuracy: 0.9054 - val_loss: 0.3078 - val_accuracy: 0.8857\n",
      "Epoch 11/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.2254 - accuracy: 0.9162 - val_loss: 0.3040 - val_accuracy: 0.8849\n",
      "Epoch 12/20\n",
      "43/43 [==============================] - 9s 211ms/step - loss: 0.2111 - accuracy: 0.9220 - val_loss: 0.2808 - val_accuracy: 0.8981\n",
      "Epoch 13/20\n",
      "43/43 [==============================] - 9s 216ms/step - loss: 0.1814 - accuracy: 0.9334 - val_loss: 0.2725 - val_accuracy: 0.9003\n",
      "Epoch 14/20\n",
      "43/43 [==============================] - 10s 222ms/step - loss: 0.1677 - accuracy: 0.9370 - val_loss: 0.2953 - val_accuracy: 0.8982\n",
      "Epoch 15/20\n",
      "43/43 [==============================] - 10s 226ms/step - loss: 0.1528 - accuracy: 0.9444 - val_loss: 0.2719 - val_accuracy: 0.9011\n",
      "Epoch 16/20\n",
      "43/43 [==============================] - 10s 222ms/step - loss: 0.1319 - accuracy: 0.9526 - val_loss: 0.2554 - val_accuracy: 0.9100\n",
      "Epoch 17/20\n",
      "43/43 [==============================] - 9s 215ms/step - loss: 0.1177 - accuracy: 0.9572 - val_loss: 0.2669 - val_accuracy: 0.9120\n",
      "Epoch 18/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.1039 - accuracy: 0.9625 - val_loss: 0.2532 - val_accuracy: 0.9130\n",
      "Epoch 19/20\n",
      "43/43 [==============================] - 10s 224ms/step - loss: 0.0983 - accuracy: 0.9653 - val_loss: 0.2555 - val_accuracy: 0.9172\n",
      "Epoch 20/20\n",
      "43/43 [==============================] - 9s 222ms/step - loss: 0.0864 - accuracy: 0.9692 - val_loss: 0.2534 - val_accuracy: 0.9160\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(train_pad, train_labels, epochs=N_EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_pad, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "969cd373-7ca1-46ed-ae94-62d7b2c29e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 2s 88ms/step - loss: 1.4364 - accuracy: 0.7074\n",
      "Test Set Accuracy = 0.7074\n"
     ]
    }
   ],
   "source": [
    " _, accuracy = model.evaluate(test_pad, test_labels, batch_size=BATCH_SIZE)\n",
    "print(\"Test Set Accuracy = {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb434b75-abe4-4f1e-bc11-10c125d826e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Glove 200: 65%\n",
    "### Twitter 200: 62%\n",
    "\n",
    "### Max sentence length = 100\n",
    "### Twitter 200: 62%, VAL ACCURACY ~87%\n",
    "### Glove 200: 70%, val accuracy 82-85%\n",
    "\n",
    "### Max vocab size\n",
    "### Glove 200: 69.38%, val accuracy 82-88%\n",
    "### Glove 100: 70%\n",
    "\n",
    "### Bidirectional LSTM\n",
    "### Glove 100: 72.8%, val accuracy 89-94%\n",
    "\n",
    "### Max sentence length = 50\n",
    "### Glove 100: 72.8%\n",
    "\n",
    "### Batch Size = 2000 (previously 250)\n",
    "### Glove 100: 73.46%\n",
    "\n",
    "### Batch size = 500\n",
    "### Glove 100: 73.9\n",
    "\n",
    "### Batch size = 100\n",
    "### Glove 100: 73.5%%\n",
    "\n",
    "### Epochs = 20\n",
    "### Glove 100: 73.9%\n",
    "\n",
    "### Max sentences = 30\n",
    "### Glove 100: 70.7%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66da1bc8-9d83-4b7e-a5c1-55aa281cd160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 36545, 1: 18272, 2: 18272, 0: 9136})\n",
      "Combined:Counter({1: 18272, 2: 18272, 3: 18272, 0: 9136})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy={0:round(36545*0.25), 1: round(36545*0.5), 2: round(36545*0.5)})\n",
    "undersample = RandomUnderSampler(sampling_strategy={3: round(36545*0.5)})\n",
    "\n",
    "train_over, training_over_labels = oversample.fit_resample(train, train['Stance'])\n",
    "\n",
    "print(Counter(training_over_labels))\n",
    "\n",
    "train_combined_sampling, training_combined_sampling_labels = undersample.fit_resample(train_over, training_over_labels)\n",
    "\n",
    "print(f\"Combined:{Counter(training_combined_sampling_labels)}\")\n",
    "\n",
    "training_combined_labels = to_categorical(np.array(training_combined_sampling_labels))\n",
    "testing_labels = to_categorical(np.array(test['Stance']))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['combinedText'])\n",
    "test_sequence = tokenizer.texts_to_sequences(test['combinedText'])\n",
    "train_combined_sequence = tokenizer.texts_to_sequences(train_combined_sampling['combinedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a00dbb-1ddb-412a-9e54-76db870f279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = '../data/glove/glove.6B.100d.txt'\n",
    "out_file = '../data/glove/glove.6B.100d.word2vec.txt'\n",
    "\n",
    "glove2word2vec(in_file, out_file)\n",
    "w2v = KeyedVectors.load_word2vec_format(out_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e08a6b-3ace-4b34-b3eb-35903c0dd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index.keys()\n",
    "# Add one because index 0 is reserved and isn't assigned to any word\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "embedding_matrix = np.zeros((len(vocab)+1, EMBEDDING_DIM))\n",
    "\n",
    "embedding_matrix[0] = np.random.random((1, EMBEDDING_DIM))\n",
    "for i, word in enumerate(vocab, 1):\n",
    "    try:\n",
    "        embedding_matrix[i] = w2v[word]\n",
    "    except KeyError as e:\n",
    "        embedding_matrix[i] = np.random.random((1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eea5b24-15ec-433b-a5e1-a70b0119a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_pad = pad_sequences(train_combined_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(test_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "train_combined_pad, val_combined_pad, train_combined_labels, val_combined_labels = train_test_split(train_combined_pad, training_combined_labels, random_state = 42, test_size = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66719526-3aad-4c61-bf8c-9bf9ef577675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model is: epoch=10, batch_size=100, and dropout=0.2\n",
      "WARNING:tensorflow:From /srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /srv/jupyter_python3-extras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 54359 samples, validate on 9593 samples\n",
      "Epoch 1/10\n",
      "54359/54359 [==============================] - 287s 5ms/step - loss: 0.9605 - accuracy: 0.5836 - val_loss: 0.6922 - val_accuracy: 0.7195\n",
      "Epoch 2/10\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.5186 - accuracy: 0.8038 - val_loss: 0.3824 - val_accuracy: 0.8626\n",
      "Epoch 3/10\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.3074 - accuracy: 0.8923 - val_loss: 0.2727 - val_accuracy: 0.9047\n",
      "Epoch 4/10\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.2020 - accuracy: 0.9316 - val_loss: 0.1914 - val_accuracy: 0.9374\n",
      "Epoch 5/10\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.1347 - accuracy: 0.9563 - val_loss: 0.1402 - val_accuracy: 0.9558\n",
      "Epoch 6/10\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.0983 - accuracy: 0.9686 - val_loss: 0.1140 - val_accuracy: 0.9617\n",
      "Epoch 7/10\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.0704 - accuracy: 0.9778 - val_loss: 0.0996 - val_accuracy: 0.9694\n",
      "Epoch 8/10\n",
      " 2300/54359 [>.............................] - ETA: 4:14 - loss: 0.0594 - accuracy: 0.9778"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "epochs = [10, 20]\n",
    "batch_sizes = [100, 500, 1000, 2000]\n",
    "dropouts = [0.2, 0.3, 0.4]\n",
    "\n",
    "best_model = Sequential()\n",
    "# best_accuracy = 0\n",
    "best_accuracy = 0.7060\n",
    "for epoch in epochs:\n",
    "    for batch_size in batch_sizes:\n",
    "        for dropout in dropouts:\n",
    "            if (epoch !=5 or (epoch == 5 and batch_size == 1000) or (epoch == 5 and batch_size == 2000)):\n",
    "                keras.backend.clear_session()\n",
    "\n",
    "                print(f\"Current Model is: epoch={epoch}, batch_size={batch_size}, and dropout={dropout}\")\n",
    "\n",
    "                model = Sequential()\n",
    "\n",
    "                model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                                    output_dim=EMBEDDING_DIM, \n",
    "                                    input_length = MAX_SENT_LEN,\n",
    "                                    weights = [embedding_matrix], \n",
    "                                    trainable=False, \n",
    "                                    name='word_embedding_layer',\n",
    "                                    mask_zero=True))\n",
    "\n",
    "                model.add(Bidirectional(LSTM(120, return_sequences = False)))\n",
    "                # model.add(Flatten())\n",
    "                model.add(Dropout(dropout))\n",
    "\n",
    "                model.add(Dense(4, activation = 'softmax', name='softmax_output_layer'))\n",
    "\n",
    "                # model.add(Dense(2, activation = 'softmax', name='softmax_output_layer', activity_regularizer=l2(L2)))\n",
    "\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer='adam',\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "    #             model.summary()\n",
    "                fit = model.fit(train_combined_pad, train_combined_labels, epochs=epoch, batch_size=batch_size, validation_data=(val_combined_pad, val_combined_labels))\n",
    "                _, accuracy = model.evaluate(test_pad, testing_labels, batch_size=batch_size)\n",
    "                print(\"Test Set Accuracy = {:.4f}\".format(accuracy))\n",
    "\n",
    "                if (accuracy > best_accuracy):\n",
    "                    best_accuracy = accuracy\n",
    "                    best_model = model\n",
    "                    best_model_string = f\"Best Model has epoch={epoch}, batch_size={batch_size}, and dropout={dropout}\"\n",
    "                    print(best_model_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c12e01-c0ac-4806-872b-135c9c2d55d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
