{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc749cdf-c5de-4090-ba05-ae60c1e33ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation, Flatten, LSTM, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6e6a4-9634-4f13-b0f2-865b0e665fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some hyperparameters that can be tuned\n",
    "MAX_SENT_LEN = 200\n",
    "MAX_VOCAB_SIZE = 100000\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 1000\n",
    "N_EPOCHS = 20\n",
    "DROPOUT = 0.0001\n",
    "L2 = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772b3d4-77d7-4272-8c62-dba887060227",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../fnc-1-baseline-master/data/train.csv')\n",
    "test = pd.read_csv('../fnc-1-baseline-master/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0a741ec7-5b28-47e8-b88f-fe2a72369e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(np.array(train['Stance']))\n",
    "test_labels = to_categorical(np.array(test['Stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b972f259-a0c0-4b69-bf28-5acdb3e28d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train['combinedText'])\n",
    "train_sequence = tokenizer.texts_to_sequences(train['combinedText'])\n",
    "test_sequence = tokenizer.texts_to_sequences(test['combinedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba04470-d93b-4e2b-87d7-b099b8ec41e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5958c5dc-b621-4afa-a3f2-060c654507ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(test_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "\n",
    "# pad sequences manually\n",
    "# def preprocessing(headlines, bodies, stances, tokenizer):\n",
    "#     # Convert the sequence of words to sequnce of indices\n",
    "#     X = tokenizer.texts_to_sequences([' '.join((headline + \"<>\" + body)[:MAX_SENT_LEN]) for headline in headlines for body in bodies])\n",
    "#     X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "#     return X, y\n",
    "\n",
    "# X_train, y_train = preprocessing(headlines, bodies, stance, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6a1ad8e-549e-4369-8ad7-d2bbed113a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/jupyter_python3-extras/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "in_file = '../fnc-1-baseline-master/data/glove_wiki/glove.6B.100d.txt'\n",
    "out_file = '../fnc-1-baseline-master/data/glove_wiki/glove.6B.100d.word2vec.txt'\n",
    "\n",
    "glove2word2vec(in_file, out_file)\n",
    "w2v = KeyedVectors.load_word2vec_format(out_file, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8b3c5fa-cedf-4895-ad75-bcaef4d0ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = '../fnc-1-baseline-master/data/glove_twitter/glove.27B.200d.txt'\n",
    "# outp = '../fnc-1-baseline-master/data/glove_twitter/glove.27B.200d.word2vec.txt'\n",
    "\n",
    "# glove2word2vec(inp, outp)\n",
    "# w2v_twitter = KeyedVectors.load_word2vec_format(outp, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "feacb695-e41b-4690-8026-5cc7f2aaf9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index.keys()\n",
    "# Add one because index 0 is reserved and isn't assigned to any word\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "embedding_matrix = np.zeros((len(vocab)+1, EMBEDDING_DIM))\n",
    "\n",
    "embedding_matrix[0] = np.random.random((1, EMBEDDING_DIM))\n",
    "for i, word in enumerate(vocab, 1):\n",
    "    try:\n",
    "        embedding_matrix[i] = w2v[word]\n",
    "    except KeyError as e:\n",
    "        embedding_matrix[i] = np.random.random((1, EMBEDDING_DIM))\n",
    "        \n",
    "# from a3\n",
    "# embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM))   \n",
    "# for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
    "#     try:\n",
    "#         embeddings_vector = word_embeddings[word]\n",
    "#     except KeyError:\n",
    "#         embeddings_vector = None\n",
    "#     if embeddings_vector is not None:\n",
    "#         embeddings_matrix[i] = embeddings_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b0f044dd-7733-4145-8b6e-ae49091f0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad, val_pad, train_labels, val_labels = train_test_split(train_pad, train_labels, random_state = 42, test_size = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fa6a8cf-aa4f-478f-8446-863ca753baa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6372"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa6f26de-d670-45d0-a6de-56bdaa21dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " word_embedding_layer (Embed  (None, 30, 100)          2787600   \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 240)              212160    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 240)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 240)               0         \n",
      "                                                                 \n",
      " softmax_output_layer (Dense  (None, 4)                964       \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,724\n",
      "Trainable params: 213,124\n",
      "Non-trainable params: 2,787,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length = MAX_SENT_LEN,\n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable=False, \n",
    "                    name='word_embedding_layer',\n",
    "                    mask_zero=True))\n",
    "\n",
    "model.add(Bidirectional(LSTM(120, return_sequences = False)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Dense(4, activation = 'softmax', name='softmax_output_layer'))\n",
    "\n",
    "# model.add(Dense(2, activation = 'softmax', name='softmax_output_layer', activity_regularizer=l2(L2)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bec353d6-f337-45d7-aa03-2d7bb6bae268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "43/43 [==============================] - 16s 258ms/step - loss: 0.8021 - accuracy: 0.7302 - val_loss: 0.7522 - val_accuracy: 0.7393\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 9s 216ms/step - loss: 0.7433 - accuracy: 0.7366 - val_loss: 0.7095 - val_accuracy: 0.7488\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.6884 - accuracy: 0.7495 - val_loss: 0.6560 - val_accuracy: 0.7680\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 9s 212ms/step - loss: 0.6145 - accuracy: 0.7700 - val_loss: 0.5753 - val_accuracy: 0.7876\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 10s 221ms/step - loss: 0.5268 - accuracy: 0.8025 - val_loss: 0.5002 - val_accuracy: 0.8100\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 9s 218ms/step - loss: 0.4587 - accuracy: 0.8273 - val_loss: 0.4554 - val_accuracy: 0.8306\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 9s 220ms/step - loss: 0.3887 - accuracy: 0.8559 - val_loss: 0.3977 - val_accuracy: 0.8514\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 10s 223ms/step - loss: 0.3334 - accuracy: 0.8771 - val_loss: 0.3563 - val_accuracy: 0.8665\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 10s 227ms/step - loss: 0.2953 - accuracy: 0.8905 - val_loss: 0.3360 - val_accuracy: 0.8766\n",
      "Epoch 10/20\n",
      "43/43 [==============================] - 10s 224ms/step - loss: 0.2585 - accuracy: 0.9054 - val_loss: 0.3078 - val_accuracy: 0.8857\n",
      "Epoch 11/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.2254 - accuracy: 0.9162 - val_loss: 0.3040 - val_accuracy: 0.8849\n",
      "Epoch 12/20\n",
      "43/43 [==============================] - 9s 211ms/step - loss: 0.2111 - accuracy: 0.9220 - val_loss: 0.2808 - val_accuracy: 0.8981\n",
      "Epoch 13/20\n",
      "43/43 [==============================] - 9s 216ms/step - loss: 0.1814 - accuracy: 0.9334 - val_loss: 0.2725 - val_accuracy: 0.9003\n",
      "Epoch 14/20\n",
      "43/43 [==============================] - 10s 222ms/step - loss: 0.1677 - accuracy: 0.9370 - val_loss: 0.2953 - val_accuracy: 0.8982\n",
      "Epoch 15/20\n",
      "43/43 [==============================] - 10s 226ms/step - loss: 0.1528 - accuracy: 0.9444 - val_loss: 0.2719 - val_accuracy: 0.9011\n",
      "Epoch 16/20\n",
      "43/43 [==============================] - 10s 222ms/step - loss: 0.1319 - accuracy: 0.9526 - val_loss: 0.2554 - val_accuracy: 0.9100\n",
      "Epoch 17/20\n",
      "43/43 [==============================] - 9s 215ms/step - loss: 0.1177 - accuracy: 0.9572 - val_loss: 0.2669 - val_accuracy: 0.9120\n",
      "Epoch 18/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.1039 - accuracy: 0.9625 - val_loss: 0.2532 - val_accuracy: 0.9130\n",
      "Epoch 19/20\n",
      "43/43 [==============================] - 10s 224ms/step - loss: 0.0983 - accuracy: 0.9653 - val_loss: 0.2555 - val_accuracy: 0.9172\n",
      "Epoch 20/20\n",
      "43/43 [==============================] - 9s 222ms/step - loss: 0.0864 - accuracy: 0.9692 - val_loss: 0.2534 - val_accuracy: 0.9160\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(train_pad, train_labels, epochs=N_EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_pad, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "969cd373-7ca1-46ed-ae94-62d7b2c29e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 2s 88ms/step - loss: 1.4364 - accuracy: 0.7074\n",
      "Test Set Accuracy = 0.7074\n"
     ]
    }
   ],
   "source": [
    " _, accuracy = model.evaluate(test_pad, test_labels, batch_size=BATCH_SIZE)\n",
    "print(\"Test Set Accuracy = {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb434b75-abe4-4f1e-bc11-10c125d826e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Glove 200: 65%\n",
    "### Twitter 200: 62%\n",
    "\n",
    "### Max sentence length = 100\n",
    "### Twitter 200: 62%, VAL ACCURACY ~87%\n",
    "### Glove 200: 70%, val accuracy 82-85%\n",
    "\n",
    "### Max vocab size\n",
    "### Glove 200: 69.38%, val accuracy 82-88%\n",
    "### Glove 100: 70%\n",
    "\n",
    "### Bidirectional LSTM\n",
    "### Glove 100: 72.8%, val accuracy 89-94%\n",
    "\n",
    "### Max sentence length = 50\n",
    "### Glove 100: 72.8%\n",
    "\n",
    "### Batch Size = 2000 (previously 250)\n",
    "### Glove 100: 73.46%\n",
    "\n",
    "### Batch size = 500\n",
    "### Glove 100: 73.9\n",
    "\n",
    "### Batch size = 100\n",
    "### Glove 100: 73.5%%\n",
    "\n",
    "### Epochs = 20\n",
    "### Glove 100: 73.9%\n",
    "\n",
    "### Max sentences = 30\n",
    "### Glove 100: 70.7%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
