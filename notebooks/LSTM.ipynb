{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc749cdf-c5de-4090-ba05-ae60c1e33ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation, Flatten, LSTM, Bidirectional\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ea6e6a4-9634-4f13-b0f2-865b0e665fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some hyperparameters that can be tuned\n",
    "MAX_SENT_LEN = 200 \n",
    "MAX_VOCAB_SIZE = 100000\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 500\n",
    "N_EPOCHS = 20\n",
    "DROPOUT = 0.0001\n",
    "L2 = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5772b3d4-77d7-4272-8c62-dba887060227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('../fnc-1-baseline-master/data/train.csv')\n",
    "# test = pd.read_csv('../fnc-1-baseline-master/data/test.csv')\n",
    "\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a741ec7-5b28-47e8-b88f-fe2a72369e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(np.array(train['Stance']))\n",
    "# test_labels = to_categorical(np.array(test['Stance']))\n",
    "testing_labels = to_categorical(np.array(test['Stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b972f259-a0c0-4b69-bf28-5acdb3e28d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(train['combinedText'])\n",
    "train_sequence = tokenizer.texts_to_sequences(train['combinedText'])\n",
    "test_sequence = tokenizer.texts_to_sequences(test['combinedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba04470-d93b-4e2b-87d7-b099b8ec41e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5958c5dc-b621-4afa-a3f2-060c654507ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(test_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "\n",
    "# pad sequences manually\n",
    "# def preprocessing(headlines, bodies, stances, tokenizer):\n",
    "#     # Convert the sequence of words to sequnce of indices\n",
    "#     X = tokenizer.texts_to_sequences([' '.join((headline + \"<>\" + body)[:MAX_SENT_LEN]) for headline in headlines for body in bodies])\n",
    "#     X = pad_sequences(X, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "#     return X, y\n",
    "\n",
    "# X_train, y_train = preprocessing(headlines, bodies, stance, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a1ad8e-549e-4369-8ad7-d2bbed113a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_file = '../fnc-1-baseline-master/data/glove_wiki/glove.6B.100d.txt'\n",
    "# out_file = '../fnc-1-baseline-master/data/glove_wiki/glove.6B.100d.word2vec.txt'\n",
    "\n",
    "in_file = '../data/glove/glove.6B.100d.txt'\n",
    "out_file = '../data/glove/glove.6B.100d.word2vec.txt'\n",
    "\n",
    "glove2word2vec(in_file, out_file)\n",
    "w2v = KeyedVectors.load_word2vec_format(out_file, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8b3c5fa-cedf-4895-ad75-bcaef4d0ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = '../fnc-1-baseline-master/data/glove_twitter/glove.27B.200d.txt'\n",
    "# outp = '../fnc-1-baseline-master/data/glove_twitter/glove.27B.200d.word2vec.txt'\n",
    "\n",
    "# glove2word2vec(inp, outp)\n",
    "# w2v_twitter = KeyedVectors.load_word2vec_format(outp, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feacb695-e41b-4690-8026-5cc7f2aaf9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index.keys()\n",
    "# Add one because index 0 is reserved and isn't assigned to any word\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "embedding_matrix = np.zeros((len(vocab)+1, EMBEDDING_DIM))\n",
    "\n",
    "embedding_matrix[0] = np.random.random((1, EMBEDDING_DIM))\n",
    "for i, word in enumerate(vocab, 1):\n",
    "    try:\n",
    "        embedding_matrix[i] = w2v[word]\n",
    "    except KeyError as e:\n",
    "        embedding_matrix[i] = np.random.random((1, EMBEDDING_DIM))\n",
    "        \n",
    "# from a3\n",
    "# embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, EMBEDDING_DIM))   \n",
    "# for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding\n",
    "#     try:\n",
    "#         embeddings_vector = word_embeddings[word]\n",
    "#     except KeyError:\n",
    "#         embeddings_vector = None\n",
    "#     if embeddings_vector is not None:\n",
    "#         embeddings_matrix[i] = embeddings_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0f044dd-7733-4145-8b6e-ae49091f0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad, val_pad, train_labels, val_labels = train_test_split(train_pad, train_labels, random_state = 42, test_size = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0fa6a8cf-aa4f-478f-8446-863ca753baa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6372"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa6f26de-d670-45d0-a6de-56bdaa21dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " word_embedding_layer (Embed  (None, 30, 100)          2787600   \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 240)              212160    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 240)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 240)               0         \n",
      "                                                                 \n",
      " softmax_output_layer (Dense  (None, 4)                964       \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,000,724\n",
      "Trainable params: 213,124\n",
      "Non-trainable params: 2,787,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "# LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length = MAX_SENT_LEN,\n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable=False, \n",
    "                    name='word_embedding_layer',\n",
    "                    mask_zero=True))\n",
    "\n",
    "model.add(Bidirectional(LSTM(120, return_sequences = False)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Dense(4, activation = 'softmax', name='softmax_output_layer'))\n",
    "\n",
    "# model.add(Dense(2, activation = 'softmax', name='softmax_output_layer', activity_regularizer=l2(L2)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bec353d6-f337-45d7-aa03-2d7bb6bae268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "43/43 [==============================] - 16s 258ms/step - loss: 0.8021 - accuracy: 0.7302 - val_loss: 0.7522 - val_accuracy: 0.7393\n",
      "Epoch 2/20\n",
      "43/43 [==============================] - 9s 216ms/step - loss: 0.7433 - accuracy: 0.7366 - val_loss: 0.7095 - val_accuracy: 0.7488\n",
      "Epoch 3/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.6884 - accuracy: 0.7495 - val_loss: 0.6560 - val_accuracy: 0.7680\n",
      "Epoch 4/20\n",
      "43/43 [==============================] - 9s 212ms/step - loss: 0.6145 - accuracy: 0.7700 - val_loss: 0.5753 - val_accuracy: 0.7876\n",
      "Epoch 5/20\n",
      "43/43 [==============================] - 10s 221ms/step - loss: 0.5268 - accuracy: 0.8025 - val_loss: 0.5002 - val_accuracy: 0.8100\n",
      "Epoch 6/20\n",
      "43/43 [==============================] - 9s 218ms/step - loss: 0.4587 - accuracy: 0.8273 - val_loss: 0.4554 - val_accuracy: 0.8306\n",
      "Epoch 7/20\n",
      "43/43 [==============================] - 9s 220ms/step - loss: 0.3887 - accuracy: 0.8559 - val_loss: 0.3977 - val_accuracy: 0.8514\n",
      "Epoch 8/20\n",
      "43/43 [==============================] - 10s 223ms/step - loss: 0.3334 - accuracy: 0.8771 - val_loss: 0.3563 - val_accuracy: 0.8665\n",
      "Epoch 9/20\n",
      "43/43 [==============================] - 10s 227ms/step - loss: 0.2953 - accuracy: 0.8905 - val_loss: 0.3360 - val_accuracy: 0.8766\n",
      "Epoch 10/20\n",
      "43/43 [==============================] - 10s 224ms/step - loss: 0.2585 - accuracy: 0.9054 - val_loss: 0.3078 - val_accuracy: 0.8857\n",
      "Epoch 11/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.2254 - accuracy: 0.9162 - val_loss: 0.3040 - val_accuracy: 0.8849\n",
      "Epoch 12/20\n",
      "43/43 [==============================] - 9s 211ms/step - loss: 0.2111 - accuracy: 0.9220 - val_loss: 0.2808 - val_accuracy: 0.8981\n",
      "Epoch 13/20\n",
      "43/43 [==============================] - 9s 216ms/step - loss: 0.1814 - accuracy: 0.9334 - val_loss: 0.2725 - val_accuracy: 0.9003\n",
      "Epoch 14/20\n",
      "43/43 [==============================] - 10s 222ms/step - loss: 0.1677 - accuracy: 0.9370 - val_loss: 0.2953 - val_accuracy: 0.8982\n",
      "Epoch 15/20\n",
      "43/43 [==============================] - 10s 226ms/step - loss: 0.1528 - accuracy: 0.9444 - val_loss: 0.2719 - val_accuracy: 0.9011\n",
      "Epoch 16/20\n",
      "43/43 [==============================] - 10s 222ms/step - loss: 0.1319 - accuracy: 0.9526 - val_loss: 0.2554 - val_accuracy: 0.9100\n",
      "Epoch 17/20\n",
      "43/43 [==============================] - 9s 215ms/step - loss: 0.1177 - accuracy: 0.9572 - val_loss: 0.2669 - val_accuracy: 0.9120\n",
      "Epoch 18/20\n",
      "43/43 [==============================] - 9s 217ms/step - loss: 0.1039 - accuracy: 0.9625 - val_loss: 0.2532 - val_accuracy: 0.9130\n",
      "Epoch 19/20\n",
      "43/43 [==============================] - 10s 224ms/step - loss: 0.0983 - accuracy: 0.9653 - val_loss: 0.2555 - val_accuracy: 0.9172\n",
      "Epoch 20/20\n",
      "43/43 [==============================] - 9s 222ms/step - loss: 0.0864 - accuracy: 0.9692 - val_loss: 0.2534 - val_accuracy: 0.9160\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(train_pad, train_labels, epochs=N_EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_pad, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "969cd373-7ca1-46ed-ae94-62d7b2c29e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 2s 88ms/step - loss: 1.4364 - accuracy: 0.7074\n",
      "Test Set Accuracy = 0.7074\n"
     ]
    }
   ],
   "source": [
    " _, accuracy = model.evaluate(test_pad, test_labels, batch_size=BATCH_SIZE)\n",
    "print(\"Test Set Accuracy = {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb434b75-abe4-4f1e-bc11-10c125d826e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Glove 200: 65%\n",
    "### Twitter 200: 62%\n",
    "\n",
    "### Max sentence length = 100\n",
    "### Twitter 200: 62%, VAL ACCURACY ~87%\n",
    "### Glove 200: 70%, val accuracy 82-85%\n",
    "\n",
    "### Max vocab size\n",
    "### Glove 200: 69.38%, val accuracy 82-88%\n",
    "### Glove 100: 70%\n",
    "\n",
    "### Bidirectional LSTM\n",
    "### Glove 100: 72.8%, val accuracy 89-94%\n",
    "\n",
    "### Max sentence length = 50\n",
    "### Glove 100: 72.8%\n",
    "\n",
    "### Batch Size = 2000 (previously 250)\n",
    "### Glove 100: 73.46%\n",
    "\n",
    "### Batch size = 500\n",
    "### Glove 100: 73.9\n",
    "\n",
    "### Batch size = 100\n",
    "### Glove 100: 73.5%%\n",
    "\n",
    "### Epochs = 20\n",
    "### Glove 100: 73.9%\n",
    "\n",
    "### Max sentences = 30\n",
    "### Glove 100: 70.7%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66da1bc8-9d83-4b7e-a5c1-55aa281cd160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 36545, 1: 18272, 2: 18272, 0: 9136})\n",
      "Combined:Counter({1: 18272, 2: 18272, 3: 18272, 0: 9136})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "oversample = RandomOverSampler(sampling_strategy={0:round(36545*0.25), 1: round(36545*0.5), 2: round(36545*0.5)})\n",
    "undersample = RandomUnderSampler(sampling_strategy={3: round(36545*0.5)})\n",
    "\n",
    "train_over, training_over_labels = oversample.fit_resample(train, train['Stance'])\n",
    "\n",
    "print(Counter(training_over_labels))\n",
    "\n",
    "train_combined_sampling, training_combined_sampling_labels = undersample.fit_resample(train_over, training_over_labels)\n",
    "\n",
    "print(f\"Combined:{Counter(training_combined_sampling_labels)}\")\n",
    "\n",
    "training_combined_labels = to_categorical(np.array(training_combined_sampling_labels))\n",
    "testing_labels = to_categorical(np.array(test['Stance']))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train['combinedText'])\n",
    "test_sequence = tokenizer.texts_to_sequences(test['combinedText'])\n",
    "train_combined_sequence = tokenizer.texts_to_sequences(train_combined_sampling['combinedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30a00dbb-1ddb-412a-9e54-76db870f279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = '../data/glove/glove.6B.100d.txt'\n",
    "out_file = '../data/glove/glove.6B.100d.word2vec.txt'\n",
    "\n",
    "glove2word2vec(in_file, out_file)\n",
    "w2v = KeyedVectors.load_word2vec_format(out_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e08a6b-3ace-4b34-b3eb-35903c0dd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index.keys()\n",
    "# Add one because index 0 is reserved and isn't assigned to any word\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "embedding_matrix = np.zeros((len(vocab)+1, EMBEDDING_DIM))\n",
    "\n",
    "embedding_matrix[0] = np.random.random((1, EMBEDDING_DIM))\n",
    "for i, word in enumerate(vocab, 1):\n",
    "    try:\n",
    "        embedding_matrix[i] = w2v[word]\n",
    "    except KeyError as e:\n",
    "        embedding_matrix[i] = np.random.random((1, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eea5b24-15ec-433b-a5e1-a70b0119a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_pad = pad_sequences(train_combined_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(test_sequence, maxlen=MAX_SENT_LEN, padding='post', truncating='post')\n",
    "train_combined_pad, val_combined_pad, train_combined_labels, val_combined_labels = train_test_split(train_combined_pad, training_combined_labels, random_state = 42, test_size = 0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66719526-3aad-4c61-bf8c-9bf9ef577675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model is: epoch=20, batch_size=100, and dropout=0.2\n",
      "WARNING:tensorflow:From /srv/jupyter_python3-extras/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /srv/jupyter_python3-extras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 54359 samples, validate on 9593 samples\n",
      "Epoch 1/20\n",
      "54359/54359 [==============================] - 431s 8ms/step - loss: 0.9748 - accuracy: 0.5775 - val_loss: 0.6769 - val_accuracy: 0.7268\n",
      "Epoch 2/20\n",
      "54359/54359 [==============================] - 424s 8ms/step - loss: 0.5230 - accuracy: 0.7996 - val_loss: 0.4293 - val_accuracy: 0.8432\n",
      "Epoch 3/20\n",
      "54359/54359 [==============================] - 422s 8ms/step - loss: 0.3145 - accuracy: 0.8873 - val_loss: 0.2705 - val_accuracy: 0.9044\n",
      "Epoch 4/20\n",
      "54359/54359 [==============================] - 423s 8ms/step - loss: 0.1997 - accuracy: 0.9324 - val_loss: 0.1899 - val_accuracy: 0.9371\n",
      "Epoch 5/20\n",
      "54359/54359 [==============================] - 421s 8ms/step - loss: 0.1388 - accuracy: 0.9548 - val_loss: 0.1425 - val_accuracy: 0.9542\n",
      "Epoch 6/20\n",
      "54359/54359 [==============================] - 419s 8ms/step - loss: 0.0966 - accuracy: 0.9694 - val_loss: 0.1327 - val_accuracy: 0.9617\n",
      "Epoch 7/20\n",
      "54359/54359 [==============================] - 417s 8ms/step - loss: 0.0732 - accuracy: 0.9768 - val_loss: 0.1157 - val_accuracy: 0.9640\n",
      "Epoch 8/20\n",
      "54359/54359 [==============================] - 421s 8ms/step - loss: 0.0612 - accuracy: 0.9801 - val_loss: 0.0958 - val_accuracy: 0.9732\n",
      "Epoch 9/20\n",
      "54359/54359 [==============================] - 421s 8ms/step - loss: 0.0477 - accuracy: 0.9843 - val_loss: 0.1034 - val_accuracy: 0.9715\n",
      "Epoch 10/20\n",
      "54359/54359 [==============================] - 423s 8ms/step - loss: 0.0428 - accuracy: 0.9865 - val_loss: 0.1072 - val_accuracy: 0.9695\n",
      "Epoch 11/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0353 - accuracy: 0.9884 - val_loss: 0.0996 - val_accuracy: 0.9741\n",
      "Epoch 12/20\n",
      "54359/54359 [==============================] - 424s 8ms/step - loss: 0.0348 - accuracy: 0.9889 - val_loss: 0.1008 - val_accuracy: 0.9740\n",
      "Epoch 13/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0273 - accuracy: 0.9911 - val_loss: 0.0932 - val_accuracy: 0.9794\n",
      "Epoch 14/20\n",
      "54359/54359 [==============================] - 422s 8ms/step - loss: 0.0228 - accuracy: 0.9929 - val_loss: 0.0993 - val_accuracy: 0.9770\n",
      "Epoch 15/20\n",
      "54359/54359 [==============================] - 422s 8ms/step - loss: 0.0276 - accuracy: 0.9916 - val_loss: 0.0955 - val_accuracy: 0.9783\n",
      "Epoch 16/20\n",
      "54359/54359 [==============================] - 424s 8ms/step - loss: 0.0206 - accuracy: 0.9937 - val_loss: 0.1100 - val_accuracy: 0.9753\n",
      "Epoch 17/20\n",
      "54359/54359 [==============================] - 421s 8ms/step - loss: 0.0241 - accuracy: 0.9924 - val_loss: 0.1094 - val_accuracy: 0.9735\n",
      "Epoch 18/20\n",
      "54359/54359 [==============================] - 422s 8ms/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.0832 - val_accuracy: 0.9836\n",
      "Epoch 19/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0201 - accuracy: 0.9941 - val_loss: 0.0935 - val_accuracy: 0.9786\n",
      "Epoch 20/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.0754 - val_accuracy: 0.9830\n",
      "25413/25413 [==============================] - 70s 3ms/step\n",
      "Test Set Accuracy = 0.7261\n",
      "Best Model has epoch=20, batch_size=100, and dropout=0.2\n",
      "Current Model is: epoch=20, batch_size=100, and dropout=0.3\n",
      "Train on 54359 samples, validate on 9593 samples\n",
      "Epoch 1/20\n",
      "54359/54359 [==============================] - 423s 8ms/step - loss: 0.9751 - accuracy: 0.5752 - val_loss: 0.6566 - val_accuracy: 0.7444\n",
      "Epoch 2/20\n",
      "54359/54359 [==============================] - 421s 8ms/step - loss: 0.5187 - accuracy: 0.8037 - val_loss: 0.3731 - val_accuracy: 0.8662\n",
      "Epoch 3/20\n",
      "54359/54359 [==============================] - 426s 8ms/step - loss: 0.2947 - accuracy: 0.8958 - val_loss: 0.2728 - val_accuracy: 0.8987\n",
      "Epoch 4/20\n",
      "54359/54359 [==============================] - 430s 8ms/step - loss: 0.1891 - accuracy: 0.9365 - val_loss: 0.1703 - val_accuracy: 0.9432\n",
      "Epoch 5/20\n",
      "54359/54359 [==============================] - 469s 9ms/step - loss: 0.1288 - accuracy: 0.9572 - val_loss: 0.1279 - val_accuracy: 0.9592\n",
      "Epoch 6/20\n",
      "54359/54359 [==============================] - 483s 9ms/step - loss: 0.0923 - accuracy: 0.9706 - val_loss: 0.1147 - val_accuracy: 0.9664\n",
      "Epoch 7/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0707 - accuracy: 0.9778 - val_loss: 0.1062 - val_accuracy: 0.9642\n",
      "Epoch 8/20\n",
      "54359/54359 [==============================] - 421s 8ms/step - loss: 0.0569 - accuracy: 0.9821 - val_loss: 0.0839 - val_accuracy: 0.9757\n",
      "Epoch 9/20\n",
      "54359/54359 [==============================] - 421s 8ms/step - loss: 0.0499 - accuracy: 0.9840 - val_loss: 0.1273 - val_accuracy: 0.9637\n",
      "Epoch 10/20\n",
      "54359/54359 [==============================] - 445s 8ms/step - loss: 0.0396 - accuracy: 0.9877 - val_loss: 0.0925 - val_accuracy: 0.9738\n",
      "Epoch 11/20\n",
      "54359/54359 [==============================] - 436s 8ms/step - loss: 0.0375 - accuracy: 0.9879 - val_loss: 0.0906 - val_accuracy: 0.9759\n",
      "Epoch 12/20\n",
      "54359/54359 [==============================] - 417s 8ms/step - loss: 0.0345 - accuracy: 0.9894 - val_loss: 0.0843 - val_accuracy: 0.9779\n",
      "Epoch 13/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.0769 - val_accuracy: 0.9790\n",
      "Epoch 14/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0255 - accuracy: 0.9919 - val_loss: 0.0807 - val_accuracy: 0.9805\n",
      "Epoch 15/20\n",
      "54359/54359 [==============================] - 411s 8ms/step - loss: 0.0235 - accuracy: 0.9931 - val_loss: 0.0870 - val_accuracy: 0.9800\n",
      "Epoch 16/20\n",
      "54359/54359 [==============================] - 408s 8ms/step - loss: 0.0214 - accuracy: 0.9934 - val_loss: 0.0922 - val_accuracy: 0.9792\n",
      "Epoch 17/20\n",
      "54359/54359 [==============================] - 417s 8ms/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 0.0942 - val_accuracy: 0.9771\n",
      "Epoch 18/20\n",
      "54359/54359 [==============================] - 438s 8ms/step - loss: 0.0181 - accuracy: 0.9945 - val_loss: 0.0931 - val_accuracy: 0.9792\n",
      "Epoch 19/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0131 - accuracy: 0.9961 - val_loss: 0.0717 - val_accuracy: 0.9843\n",
      "Epoch 20/20\n",
      "54359/54359 [==============================] - 436s 8ms/step - loss: 0.0208 - accuracy: 0.9938 - val_loss: 0.1051 - val_accuracy: 0.9797\n",
      "25413/25413 [==============================] - 69s 3ms/step\n",
      "Test Set Accuracy = 0.6922\n",
      "Current Model is: epoch=20, batch_size=100, and dropout=0.4\n",
      "Train on 54359 samples, validate on 9593 samples\n",
      "Epoch 1/20\n",
      "54359/54359 [==============================] - 411s 8ms/step - loss: 1.0122 - accuracy: 0.5563 - val_loss: 0.7393 - val_accuracy: 0.7020\n",
      "Epoch 2/20\n",
      "54359/54359 [==============================] - 410s 8ms/step - loss: 0.5686 - accuracy: 0.7825 - val_loss: 0.4262 - val_accuracy: 0.8467\n",
      "Epoch 3/20\n",
      "54359/54359 [==============================] - 411s 8ms/step - loss: 0.3454 - accuracy: 0.8774 - val_loss: 0.2700 - val_accuracy: 0.9117\n",
      "Epoch 4/20\n",
      "54359/54359 [==============================] - 409s 8ms/step - loss: 0.2208 - accuracy: 0.9246 - val_loss: 0.2156 - val_accuracy: 0.9319\n",
      "Epoch 5/20\n",
      "54359/54359 [==============================] - 466s 9ms/step - loss: 0.1528 - accuracy: 0.9506 - val_loss: 0.1530 - val_accuracy: 0.9534\n",
      "Epoch 6/20\n",
      "54359/54359 [==============================] - 428s 8ms/step - loss: 0.1110 - accuracy: 0.9646 - val_loss: 0.1281 - val_accuracy: 0.9601\n",
      "Epoch 7/20\n",
      "54359/54359 [==============================] - 418s 8ms/step - loss: 0.0843 - accuracy: 0.9721 - val_loss: 0.1174 - val_accuracy: 0.9650\n",
      "Epoch 8/20\n",
      "54359/54359 [==============================] - 444s 8ms/step - loss: 0.0671 - accuracy: 0.9787 - val_loss: 0.1228 - val_accuracy: 0.9623\n",
      "Epoch 9/20\n",
      "54359/54359 [==============================] - 420s 8ms/step - loss: 0.0597 - accuracy: 0.9805 - val_loss: 0.1112 - val_accuracy: 0.9701\n",
      "Epoch 10/20\n",
      "54359/54359 [==============================] - 413s 8ms/step - loss: 0.0456 - accuracy: 0.9855 - val_loss: 0.1185 - val_accuracy: 0.9707\n",
      "Epoch 11/20\n",
      "54359/54359 [==============================] - 417s 8ms/step - loss: 0.0402 - accuracy: 0.9873 - val_loss: 0.1058 - val_accuracy: 0.9757\n",
      "Epoch 12/20\n",
      "54359/54359 [==============================] - 410s 8ms/step - loss: 0.0446 - accuracy: 0.9861 - val_loss: 0.0871 - val_accuracy: 0.9774\n",
      "Epoch 13/20\n",
      "54359/54359 [==============================] - 412s 8ms/step - loss: 0.0367 - accuracy: 0.9886 - val_loss: 0.0868 - val_accuracy: 0.9781\n",
      "Epoch 14/20\n",
      "54359/54359 [==============================] - 458s 8ms/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.1057 - val_accuracy: 0.9770\n",
      "Epoch 15/20\n",
      "54359/54359 [==============================] - 413s 8ms/step - loss: 0.0295 - accuracy: 0.9912 - val_loss: 0.0929 - val_accuracy: 0.9783\n",
      "Epoch 16/20\n",
      "54359/54359 [==============================] - 411s 8ms/step - loss: 0.0302 - accuracy: 0.9908 - val_loss: 0.0791 - val_accuracy: 0.9806\n",
      "Epoch 17/20\n",
      "54359/54359 [==============================] - 411s 8ms/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 0.0747 - val_accuracy: 0.9811\n",
      "Epoch 18/20\n",
      "54359/54359 [==============================] - 413s 8ms/step - loss: 0.0241 - accuracy: 0.9928 - val_loss: 0.0792 - val_accuracy: 0.9804\n",
      "Epoch 19/20\n",
      "54359/54359 [==============================] - 411s 8ms/step - loss: 0.0237 - accuracy: 0.9929 - val_loss: 0.0737 - val_accuracy: 0.9827\n",
      "Epoch 20/20\n",
      "54359/54359 [==============================] - 411s 8ms/step - loss: 0.0174 - accuracy: 0.9947 - val_loss: 0.0806 - val_accuracy: 0.9829\n",
      "25413/25413 [==============================] - 69s 3ms/step\n",
      "Test Set Accuracy = 0.7307\n",
      "Best Model has epoch=20, batch_size=100, and dropout=0.4\n",
      "Current Model is: epoch=20, batch_size=500, and dropout=0.2\n",
      "Train on 54359 samples, validate on 9593 samples\n",
      "Epoch 1/20\n",
      "54359/54359 [==============================] - 295s 5ms/step - loss: 1.1384 - accuracy: 0.4839 - val_loss: 1.0071 - val_accuracy: 0.5621\n",
      "Epoch 2/20\n",
      "54359/54359 [==============================] - 288s 5ms/step - loss: 0.8305 - accuracy: 0.6579 - val_loss: 0.7315 - val_accuracy: 0.7011\n",
      "Epoch 3/20\n",
      "54359/54359 [==============================] - 288s 5ms/step - loss: 0.5965 - accuracy: 0.7666 - val_loss: 0.5184 - val_accuracy: 0.8052\n",
      "Epoch 4/20\n",
      "54359/54359 [==============================] - 285s 5ms/step - loss: 0.4360 - accuracy: 0.8367 - val_loss: 0.4523 - val_accuracy: 0.8337\n",
      "Epoch 5/20\n",
      "54359/54359 [==============================] - 287s 5ms/step - loss: 0.3401 - accuracy: 0.8781 - val_loss: 0.3050 - val_accuracy: 0.8947\n",
      "Epoch 6/20\n",
      "54359/54359 [==============================] - 287s 5ms/step - loss: 0.2564 - accuracy: 0.9115 - val_loss: 0.2521 - val_accuracy: 0.9130\n",
      "Epoch 7/20\n",
      "54359/54359 [==============================] - 284s 5ms/step - loss: 0.1955 - accuracy: 0.9352 - val_loss: 0.2156 - val_accuracy: 0.9302\n",
      "Epoch 8/20\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.1746 - accuracy: 0.9424 - val_loss: 0.2369 - val_accuracy: 0.9174\n",
      "Epoch 9/20\n",
      "54359/54359 [==============================] - 285s 5ms/step - loss: 0.1445 - accuracy: 0.9529 - val_loss: 0.1671 - val_accuracy: 0.9481\n",
      "Epoch 10/20\n",
      "54359/54359 [==============================] - 287s 5ms/step - loss: 0.0989 - accuracy: 0.9689 - val_loss: 0.1347 - val_accuracy: 0.9594\n",
      "Epoch 11/20\n",
      "54359/54359 [==============================] - 285s 5ms/step - loss: 0.0816 - accuracy: 0.9748 - val_loss: 0.1186 - val_accuracy: 0.9675\n",
      "Epoch 12/20\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.0630 - accuracy: 0.9807 - val_loss: 0.1171 - val_accuracy: 0.9698\n",
      "Epoch 13/20\n",
      "54359/54359 [==============================] - 285s 5ms/step - loss: 0.0515 - accuracy: 0.9843 - val_loss: 0.1092 - val_accuracy: 0.9691\n",
      "Epoch 14/20\n",
      "54359/54359 [==============================] - 285s 5ms/step - loss: 0.0463 - accuracy: 0.9859 - val_loss: 0.1085 - val_accuracy: 0.9722\n",
      "Epoch 15/20\n",
      "54359/54359 [==============================] - 284s 5ms/step - loss: 0.0544 - accuracy: 0.9827 - val_loss: 0.0930 - val_accuracy: 0.9760\n",
      "Epoch 16/20\n",
      "54359/54359 [==============================] - 284s 5ms/step - loss: 0.0315 - accuracy: 0.9906 - val_loss: 0.0929 - val_accuracy: 0.9782\n",
      "Epoch 17/20\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.0257 - accuracy: 0.9926 - val_loss: 0.1032 - val_accuracy: 0.9768\n",
      "Epoch 18/20\n",
      "54359/54359 [==============================] - 284s 5ms/step - loss: 0.0205 - accuracy: 0.9942 - val_loss: 0.1192 - val_accuracy: 0.9736\n",
      "Epoch 19/20\n",
      "54359/54359 [==============================] - 285s 5ms/step - loss: 0.0298 - accuracy: 0.9906 - val_loss: 0.0965 - val_accuracy: 0.9775\n",
      "Epoch 20/20\n",
      "54359/54359 [==============================] - 284s 5ms/step - loss: 0.0310 - accuracy: 0.9905 - val_loss: 0.1115 - val_accuracy: 0.9723\n",
      "25413/25413 [==============================] - 47s 2ms/step\n",
      "Test Set Accuracy = 0.6997\n",
      "Current Model is: epoch=20, batch_size=500, and dropout=0.3\n",
      "Train on 54359 samples, validate on 9593 samples\n",
      "Epoch 1/20\n",
      "54359/54359 [==============================] - 287s 5ms/step - loss: 1.1571 - accuracy: 0.4788 - val_loss: 1.0059 - val_accuracy: 0.5727\n",
      "Epoch 2/20\n",
      "54359/54359 [==============================] - 286s 5ms/step - loss: 0.8588 - accuracy: 0.6427 - val_loss: 0.7134 - val_accuracy: 0.7156\n",
      "Epoch 3/20\n",
      "54359/54359 [==============================] - 285s 5ms/step - loss: 0.6246 - accuracy: 0.7561 - val_loss: 0.5126 - val_accuracy: 0.8049\n",
      "Epoch 4/20\n",
      "54359/54359 [==============================] - 393s 7ms/step - loss: 0.4641 - accuracy: 0.8276 - val_loss: 0.3954 - val_accuracy: 0.8597\n",
      "Epoch 5/20\n",
      "54359/54359 [==============================] - 476s 9ms/step - loss: 0.3642 - accuracy: 0.8666 - val_loss: 0.3308 - val_accuracy: 0.8816\n",
      "Epoch 6/20\n",
      " 2000/54359 [>.............................] - ETA: 7:35 - loss: 0.3061 - accuracy: 0.8970"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "epochs = [20]\n",
    "batch_sizes = [100, 500]\n",
    "dropouts = [0.2, 0.3, 0.4]\n",
    "\n",
    "best_model = Sequential()\n",
    "# best_accuracy = 0\n",
    "best_accuracy = 0.7060\n",
    "for epoch in epochs:\n",
    "    for batch_size in batch_sizes:\n",
    "        for dropout in dropouts:\n",
    "            if (epoch !=10 or (epoch == 10 and batch_size == 1000) or (epoch == 10 and batch_size == 2000)):\n",
    "                if (epoch == 10 and batch_size==1000 and dropout == 0.2): continue\n",
    "                keras.backend.clear_session()\n",
    "\n",
    "                print(f\"Current Model is: epoch={epoch}, batch_size={batch_size}, and dropout={dropout}\")\n",
    "\n",
    "                model = Sequential()\n",
    "\n",
    "                model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                                    output_dim=EMBEDDING_DIM, \n",
    "                                    input_length = MAX_SENT_LEN,\n",
    "                                    weights = [embedding_matrix], \n",
    "                                    trainable=False, \n",
    "                                    name='word_embedding_layer',\n",
    "                                    mask_zero=True))\n",
    "\n",
    "                model.add(Bidirectional(LSTM(120, return_sequences = False)))\n",
    "                # model.add(Flatten())\n",
    "                model.add(Dropout(dropout))\n",
    "\n",
    "                model.add(Dense(4, activation = 'softmax', name='softmax_output_layer'))\n",
    "\n",
    "                # model.add(Dense(2, activation = 'softmax', name='softmax_output_layer', activity_regularizer=l2(L2)))\n",
    "\n",
    "                model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer='adam',\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "    #             model.summary()\n",
    "                fit = model.fit(train_combined_pad, train_combined_labels, epochs=epoch, batch_size=batch_size, validation_data=(val_combined_pad, val_combined_labels))\n",
    "                _, accuracy = model.evaluate(test_pad, testing_labels, batch_size=batch_size)\n",
    "                print(\"Test Set Accuracy = {:.4f}\".format(accuracy))\n",
    "\n",
    "                if (accuracy > best_accuracy):\n",
    "                    best_accuracy = accuracy\n",
    "                    best_model = model\n",
    "                    best_model_string = f\"Best Model has epoch={epoch}, batch_size={batch_size}, and dropout={dropout}\"\n",
    "                    print(best_model_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c12e01-c0ac-4806-872b-135c9c2d55d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc970562-5cb9-4b5b-b284-3d187a085da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING AND SAVING THE BEST TRAINED MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1cd07-8874-43e3-a81c-f67b8b4aabf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/jupyter_python3-extras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 42476 samples, validate on 7496 samples\n",
      "Epoch 1/20\n",
      "42476/42476 [==============================] - 65s 2ms/step - loss: 0.7950 - accuracy: 0.7244 - val_loss: 0.7241 - val_accuracy: 0.7491\n",
      "Epoch 2/20\n",
      "42476/42476 [==============================] - 63s 1ms/step - loss: 0.6974 - accuracy: 0.7483 - val_loss: 0.6459 - val_accuracy: 0.7561\n",
      "Epoch 3/20\n",
      "42476/42476 [==============================] - 63s 1ms/step - loss: 0.6151 - accuracy: 0.7668 - val_loss: 0.5593 - val_accuracy: 0.7880\n",
      "Epoch 4/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.5090 - accuracy: 0.8058 - val_loss: 0.4719 - val_accuracy: 0.8295\n",
      "Epoch 5/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.4245 - accuracy: 0.8362 - val_loss: 0.4015 - val_accuracy: 0.8527\n",
      "Epoch 6/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.3506 - accuracy: 0.8670 - val_loss: 0.3503 - val_accuracy: 0.8715\n",
      "Epoch 7/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.2927 - accuracy: 0.8906 - val_loss: 0.3071 - val_accuracy: 0.8878\n",
      "Epoch 8/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.2454 - accuracy: 0.9079 - val_loss: 0.2776 - val_accuracy: 0.8978\n",
      "Epoch 9/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.2106 - accuracy: 0.9213 - val_loss: 0.2541 - val_accuracy: 0.9085\n",
      "Epoch 10/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.1764 - accuracy: 0.9336 - val_loss: 0.2283 - val_accuracy: 0.9168\n",
      "Epoch 11/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.1499 - accuracy: 0.9451 - val_loss: 0.2205 - val_accuracy: 0.9178\n",
      "Epoch 12/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.1329 - accuracy: 0.9515 - val_loss: 0.1986 - val_accuracy: 0.9297\n",
      "Epoch 13/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.1116 - accuracy: 0.9596 - val_loss: 0.1938 - val_accuracy: 0.9309\n",
      "Epoch 14/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.0975 - accuracy: 0.9648 - val_loss: 0.2073 - val_accuracy: 0.9294\n",
      "Epoch 15/20\n",
      "42476/42476 [==============================] - 61s 1ms/step - loss: 0.0914 - accuracy: 0.9672 - val_loss: 0.1973 - val_accuracy: 0.9357\n",
      "Epoch 16/20\n",
      "42476/42476 [==============================] - 62s 1ms/step - loss: 0.0817 - accuracy: 0.9708 - val_loss: 0.1900 - val_accuracy: 0.9381\n",
      "Epoch 17/20\n",
      "20000/42476 [=============>................] - ETA: 30s - loss: 0.0658 - accuracy: 0.9772"
     ]
    }
   ],
   "source": [
    "# V1\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# LSTM\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length = MAX_SENT_LEN,\n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable=False, \n",
    "                    name='word_embedding_layer',\n",
    "                    mask_zero=True))\n",
    "\n",
    "model.add(Bidirectional(LSTM(120, return_sequences = False)))\n",
    "model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(Dense(4, activation = 'softmax', name='softmax_output_layer'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(train_pad, train_labels, epochs=N_EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_pad, val_labels))\n",
    "\n",
    "y_predict, accuracy = model.evaluate(test_pad, testing_labels, batch_size=BATCH_SIZE)\n",
    "print(\"Test Set Accuracy = {:.4f}\".format(accuracy))\n",
    "model.save('../data/LSTM_v1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0cac1-da4b-4e4f-a985-b8f0ae038c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "epoch = 20\n",
    "batch_size = 100\n",
    "dropout = 0.4\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length = 478,\n",
    "                    weights = [embedding_matrix], \n",
    "                    trainable=False, \n",
    "                    name='word_embedding_layer',\n",
    "                    mask_zero=True))\n",
    "\n",
    "model.add(Bidirectional(LSTM(120, return_sequences = False)))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(4, activation = 'softmax', name='softmax_output_layer'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(train_combined_pad, train_combined_labels, epochs=epoch, batch_size=batch_size, validation_data=(val_combined_pad, val_combined_labels))\n",
    "_, accuracy = model.evaluate(test_pad, testing_labels, batch_size=batch_size)\n",
    "print(\"Test Set Accuracy = {:.4f}\".format(accuracy))\n",
    "model.save('../data/LSTM_v2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54aefc6-9aae-4086-a2ac-588b78384cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(test_pad)\n",
    "y_pred_competition = y_predict.argmax(axis=-1)\n",
    "y_classes = []\n",
    "\n",
    "for i in range(len(y_pred_competition)):\n",
    "    if y_pred_competition[i] == 0: y_classes.append(\"disagree\")\n",
    "    \n",
    "    if y_pred_competition[i] == 1: y_classes.append(\"agree\")\n",
    "    \n",
    "    if y_pred_competition[i] == 2: y_classes.append(\"discuss\")\n",
    "    \n",
    "    if y_pred_competition[i] == 3: y_classes.append(\"unrelated\")\n",
    "    \n",
    "answers = pd.DataFrame()\n",
    "answers['Headline'] = test['Headline']\n",
    "answers['Body ID'] = test['Body ID']\n",
    "answers['Stance'] = y_classes\n",
    "answers.to_csv('../data/answer_LSTM_v1.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a973e-48b0-4a7c-8359-45699d15cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True stance')\n",
    "  plt.xlabel('Predicted stance');\n",
    "\n",
    "class_names=['Disagree', 'Agree', 'Discuss', 'Unrelated']\n",
    "cm = confusion_matrix(test['Stance'], y_pred_competition)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)\n",
    "\n",
    "accuracy = accuracy_score(test['Stance'], y_pred_competition)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "precision = precision_score(test['Stance'], y_pred_competition, average=None)\n",
    "print(f'Precision: {precision}')\n",
    "\n",
    "recall = recall_score(test['Stance'], y_pred_competition, average=None)\n",
    "print(f'Recall: {recall}')\n",
    "\n",
    "f1_scores=f1_score(test['Stance'], y_pred_competition, average=None)\n",
    "print(f'F1_Scores: {f1_scores}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
